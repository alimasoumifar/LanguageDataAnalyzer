{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Tokenizer Test ]\n",
      "\n",
      "I am attending NLP class 2 days a week at S.B.U. this Spring. ['I', 'am', 'attending', 'NLP', 'class', '', '2', 'days', 'a', 'week', 'at', 'S.B.U.', 'this', 'Spring', '.'] \n",
      "\n",
      "I don't think data-driven computational linguistics is very tough. ['I', \"don't\", 'think', 'data-driven', 'computational', 'linguistics', 'is', 'very', 'tough', '.'] \n",
      "\n",
      "@mybuddy and the drill begins again. #SemStart ['@mybuddy', 'and', 'the', 'drill', 'begins', 'again', '.', '#SemStart'] \n",
      "\n",
      "\n",
      "[ Pig Latin Test ]\n",
      "\n",
      "['I', 'am', 'attending', 'NLP', 'class', '', '2', 'days', 'a', 'week', 'at', 'S.B.U.', 'this', 'Spring', '.'] ['Iay', 'amway', 'attendingway', 'NLPay', 'assclay', '', '2', 'aysday', 'away', 'eekway', 'atway', 'S.B.U.', 'isthay', 'Springay', '.'] \n",
      "\n",
      "['I', \"don't\", 'think', 'data-driven', 'computational', 'linguistics', 'is', 'very', 'tough', '.'] ['Iay', \"don't\", 'inkthay', 'data-driven', 'omputationalcay', 'inguisticslay', 'isway', 'eryvay', 'oughtay', '.'] \n",
      "\n",
      "['@mybuddy', 'and', 'the', 'drill', 'begins', 'again', '.', '#SemStart'] ['@mybuddy', 'andway', 'ethay', 'illdray', 'eginsbay', 'againway', '.', '#SemStart'] \n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'daily547.conll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ac1a6c39b9d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# load data for 3 and 4 the adjective classifier data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mtaggedSents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetConllTags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'daily547.conll'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;31m# 3. Test Feature Extraction:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ac1a6c39b9d8>\u001b[0m in \u001b[0;36mgetConllTags\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mwordTagsPerSent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0msentNum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mwordtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0mwordtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'daily547.conll'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import re\n",
    "\n",
    "# !/usr/bin/python3\n",
    "\n",
    "# !/usr/bin/python3\n",
    "# CSE354 Sp20; Assignment 1 Template v1\n",
    "##################################################################\n",
    "\n",
    "import sys\n",
    "\n",
    "##################################################################\n",
    "# 1. Tokenizer\n",
    "\n",
    "import re  # python's regular expression package\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    # input: a single sentence as a string.\n",
    "    # output: a list of each \"word\" in the text\n",
    "    # must use regular expressions\n",
    "    # <FILL IN>\n",
    "    tokens = []\n",
    "    split = re.split(r' ', sent)\n",
    "    for word in split:\n",
    "        if not word[len(word) - 1].isalpha():\n",
    "            # is abbreviation\n",
    "            if word[len(word) - 2].isupper():\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                index = 0\n",
    "                alphas = \"\"\n",
    "                while word[index].isalpha():\n",
    "                    alphas += word[index]\n",
    "                    index += 1\n",
    "                tokens.append(alphas)\n",
    "                for index in range(index, len(word)):\n",
    "                    tokens.append(word[index])\n",
    "                    index += 1\n",
    "\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "    # <FILL IN>\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# 2. Pig Latinizer\n",
    "\n",
    "def pigLatinizer(tokens):\n",
    "    # input: tokens: a list of tokens,\n",
    "    # output: plTokens: tokens after transforming to pig latin\n",
    "    plTokens = []\n",
    "    for word in tokens:\n",
    "        almun=word.isalpha()\n",
    "        if not word.isalpha():\n",
    "            plTokens.append(word)\n",
    "        elif word[0].islower():\n",
    "            if word[0].lower() in \"aeiou\":\n",
    "                plTokens.append(word + \"way\")\n",
    "            else:\n",
    "                index = 1\n",
    "                char = word[index].lower()\n",
    "                build=word[0]\n",
    "                while char not in \"aeiou\" and index < len(word) - 1:\n",
    "                    build += char\n",
    "                    index += 1\n",
    "                    char = word[index].lower()\n",
    "                plTokens.append(word[index:] + build + \"ay\")\n",
    "        else:\n",
    "            plTokens.append(word+\"ay\")\n",
    "\n",
    "    return plTokens\n",
    "\n",
    "##################################################################\n",
    "# 3. Feature Extractor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getFeaturesForTokens(tokens, wordToIndex):\n",
    "    # input: tokens: a list of tokens,\n",
    "    # wordToIndex: dict mapping 'word' to an index in the feature list.\n",
    "    # output: list of lists (or np.array) of k feature values for the given target\n",
    "\n",
    "    num_words = len(tokens)\n",
    "    featuresPerTarget = list()  # holds arrays of feature per word\n",
    "    for targetI in range(num_words):\n",
    "        index=wordToIndex(tokens[targetI]);\n",
    "        # <FILL IN>\n",
    "        featuresPerTarget[index] = [0]*((3*num_words)+2)\n",
    "        next_word , prev_word=None , None\n",
    "        vowels, consonants = 0, 0\n",
    "        word=tokens[targetI]\n",
    "        featuresPerTarget[index][targetI+num_words+2]=1\n",
    "        if targetI > 0:\n",
    "            featuresPerTarget[targetI][targetI-1]=1\n",
    "        if targetI < num_words-1:\n",
    "            next_word= tokens[targetI+1]\n",
    "            featuresPerTarget[index][2+targetI+1+(2*num_words)]=1\n",
    "        for char in word:\n",
    "            if char in \"aeiou\":\n",
    "                vowels += 1\n",
    "            else:\n",
    "                consonants +=1\n",
    "        featuresPerTarget[index][0]=vowels\n",
    "        featuresPerTarget[index][1]=consonants\n",
    "        \n",
    "        pass\n",
    "    \n",
    "        # \n",
    "\n",
    "    return featuresPerTarget  # a (num_words x k) matrix\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# 4. Adjective Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "                                                  \n",
    "def trainAdjectiveClassifier(features, adjs):\n",
    "    # inputs: features: feature vectors (i.e. X)\n",
    "    #        adjs: whether adjective or not: [0, 1] (i.e. y)\n",
    "    # output: model -- a trained sklearn.linear_model.LogisticRegression object\n",
    "\n",
    "    model = None\n",
    "    # <FILL IN>\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "## Main and provided complete methods\n",
    "## Do not edit.\n",
    "## If necessary, write your own main, but then make sure to replace\n",
    "## and test with this before you submit.\n",
    "##\n",
    "## Note: Tests below will be a subset of those used to test your\n",
    "##       code for grading.\n",
    "\n",
    "def getConllTags(filename):\n",
    "    # input: filename for a conll style parts of speech tagged file\n",
    "    # output: a list of list of tuples\n",
    "    #        representing [[[word1, tag1], [word2, tag2]]]\n",
    "    wordTagsPerSent = [[]]\n",
    "    sentNum = 0\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        for wordtag in f:\n",
    "            wordtag = wordtag.strip()\n",
    "            if wordtag:  # still reading current sentence\n",
    "                (word, tag) = wordtag.split(\"\\t\")\n",
    "                wordTagsPerSent[sentNum].append((word, tag))\n",
    "            else:  # new sentence\n",
    "                wordTagsPerSent.append([])\n",
    "                sentNum += 1\n",
    "    return wordTagsPerSent\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    # Data for 1 and 2\n",
    "    testSents = ['I am attending NLP class 2 days a week at S.B.U. this Spring.',\n",
    "                 \"I don't think data-driven computational linguistics is very tough.\",\n",
    "                 '@mybuddy and the drill begins again. #SemStart']\n",
    "\n",
    "    # 1. Test Tokenizer:\n",
    "    print(\"\\n[ Tokenizer Test ]\\n\")\n",
    "    tokenizedSents = []\n",
    "    for s in testSents:\n",
    "        tokenizedS = tokenize(s)\n",
    "        print(s, tokenizedS, \"\\n\")\n",
    "        tokenizedSents.append(tokenizedS)\n",
    "\n",
    "    # 2. Test Pig Latinizer:\n",
    "    print(\"\\n[ Pig Latin Test ]\\n\")\n",
    "    for ts in tokenizedSents:\n",
    "        print(ts, pigLatinizer(ts), \"\\n\")\n",
    "\n",
    "    # load data for 3 and 4 the adjective classifier data:\n",
    "    taggedSents = getConllTags('daily547.conll')\n",
    "\n",
    "    # 3. Test Feature Extraction:\n",
    "    print(\"\\n[ Feature Extraction Test ]\\n\")\n",
    "    # first make word to index mapping:\n",
    "    wordToIndex = set()  # maps words to an index\n",
    "    for sent in taggedSents:\n",
    "        if sent:\n",
    "            words, tags = zip(*sent)  # splits [(w, t), (w, t)] into [w, w], [t, t]\n",
    "            wordToIndex |= set(words)  # union of the words into the set\n",
    "    print(\"  [Read \", len(taggedSents), \" Sentences]\")\n",
    "    # turn set into dictionary: word: index\n",
    "    wordToIndex = {w: i for i, w in enumerate(wordToIndex)}\n",
    "\n",
    "    # Next, call Feature extraction per sentence\n",
    "    sentXs = []\n",
    "    sentYs = []\n",
    "    print(\"  [Extracting Features]\")\n",
    "    for sent in taggedSents:\n",
    "        if sent:\n",
    "            words, tags = zip(*sent)\n",
    "            sentXs.append(getFeaturesForTokens(words, wordToIndex))\n",
    "            sentYs.append([1 if t == 'A' else 0 for t in tags])\n",
    "    # test sentences\n",
    "    print(\"\\n\", taggedSents[5], \"\\n\", sentXs[5], \"\\n\")\n",
    "    print(taggedSents[192], \"\\n\", sentXs[192], \"\\n\")\n",
    "\n",
    "    # 4. Test Classifier Model Building\n",
    "    print(\"\\n[ Feature Extraction Test ]\\n\")\n",
    "    # setup train/test:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # flatten by word rather than sent:\n",
    "    X = [j for i in sentXs for j in i]\n",
    "    y = [j for i in sentYs for j in i]\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=0.20,\n",
    "                                                            random_state=42)\n",
    "    except ValueError:\n",
    "        print(\"\\nLooks like you haven't implemented feature extraction yet.\")\n",
    "        print(\"[Ending test early]\")\n",
    "        sys.exit(1)\n",
    "    print(\"  [Broke into training/test. X_train is \", X_train.shape, \"]\")\n",
    "    # Train the model.\n",
    "    print(\"  [Training the model]\")\n",
    "    tagger = trainAdjectiveClassifier(X_train, y_train)\n",
    "    print(\"  [Done]\")\n",
    "\n",
    "    # Test the tagger.\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # get predictions:\n",
    "    y_pred = tagger.predict(X_test)\n",
    "    # compute accuracy:\n",
    "    leny = len(y_test)\n",
    "    print(\"test n: \", leny)\n",
    "    acc = np.sum([1 if (y_pred[i] == y_test[i]) else 0 for i in range(leny)]) / leny\n",
    "    print(\"Accuracy: %.4f\" % acc)\n",
    "    print(classification_report(y_test, y_pred, ['not_adj', 'adjective']))\n",
    "\n",
    "\n",
    "\n",
    "                                                  \n",
    "                                                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
